[hadoop]
1 hadoop/bin/添加到$PATH中
2 modify conf

core-site.xml

<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
<property>
<name>fs.default.name</name>
<value>hdfs://192.168.211.129:9000</value>
</property>
<property>
<name>hadoop.tmp.dir</name>
<value>/home/hadoop/hadooptmp</value>
</property>
</configuration>

mapred-site.xml

<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
<property>
<name>mapred.job.tracker</name>
<value>192.168.211.129:9001</value>
</property>
</configuration>

hdfs-site.xml

<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
<property>
<name>dfs.replication</name>
<value>1</value>
</property>
</configuration>

master中指定的SNN节点和slaves中指定的从节点位置均为本地

[hadoop@promote conf]$ cat masters   
192.168.211.129  
[hadoop@promote conf]$ cat slaves   
192.168.211.129  

3 初始化HDFS文件系统
  特别要注意的是，Hadoop并不识别带“_”的主机名，所以如果你的主机名带有“_”，一定要进行修改，修改方式参照http://blog.csdn.net/a19881029/article/details/20485079。 
  hadoop namenode -format  

4 启动Hadoop
  start-all.sh   
  
5 查看

  jps  
    2099 SecondaryNameNode  
    2184 JobTracker  
    1976 DataNode  
    2365 Jps  
    1877 NameNode  
    2289 TaskTracker  
  
  log: hadoop dfsadmin -report 
  再看看日志文件中有没有报错，如果没有报错，说明Hadoop已经启动成功了
  
  ui: hadoop nodes: http://116.211.15.212:50070/  status: http://116.211.15.212:50090/
  
[spark]

1 配置环境变量
vim ~/.bashrc

export JAVA_HOME=/home/yy/jdk1.8
export SCALA_HOME=/home/yy/scala
export SPARK_HOME=/home/yy/spark-1.2.0-bin-hadoop2.4
export PATH=$PATH:$JAVA_HOME/bin:$SCALA_HOME/bin:$SPARK_HOME/bin:$SPARK_HOME/sbin

2 修改spark配置
进入spark-1.2.0-bin-hadoop2.4/conf

cp spark-env.sh.template spark-env.sh
cp slaves.template slaves

conf/spark-env.sh:
export JAVA_HOME=/opt/jdk1.7.0_25/
export SPARK_HOME=/data1/app/spark/spark-2.0.0-bin-hadoop2.7/
export SPARK_MASTER_IP=localhost.localdomain
export SPARK_WORKER_MEMORY=2g
export HADOOP_CONF_DIR=${SPARK_HOME}/conf/

编辑slaves
添加上你的对应信息，所有的集群的机器：
conf/slaves:
localhost.localdomain

编辑/etc/hosts
116.211.15.212 localhost.localdomain

!!!: hostname只能绑定到一个ip上


3 master:  ./sbin/start-master.sh

http://116.211.15.212:8080/
URL: spark://116.211.15.212:7077
REST URL: spark://116.211.15.212:6066 (cluster mode)

4 worker: ./bin/spark-class org.apache.spark.deploy.worker.Worker spark://localhost.localdomain:7077
查看jobs http://116.211.15.212:4040/



